{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T03:23:10.178421Z",
     "start_time": "2021-02-09T03:23:07.398689Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pause, sys\n",
    "\n",
    "# 크롤링\n",
    "def crawling():\n",
    "    \n",
    "    # 섹션 별 아이디 설정\n",
    "    Section_ID = {\"Political\":\"100\", \"Economic\":\"101\", \"Society\":\"102\", \"Culture\":\"103\", \"International\":\"104\", \"Science\":\"105\"}\n",
    "\n",
    "    # 시간 별 크롤링 시간 측정\n",
    "    total_crawling_time = 0\n",
    "    now = datetime.now()\n",
    "    print(\"Start Crawling :\", now.date(), now.hour)\n",
    "    \n",
    "    # 파일 저장 경로 설정\n",
    "    date = \"\".join(str(now.date()).split('-'))\n",
    "    directory = \"C:\\\\Users\\\\cjy89\\\\NLP\\\\Project_news_crawling\\\\Naver\\\\\" + str(date)\n",
    "    if not os.path.isdir(directory):\n",
    "        os.makedirs(directory)\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # 섹션 별로 크롤링\n",
    "    for section in Section_ID:\n",
    "\n",
    "        # 섹션 별 크롤링 시간 측정 start\n",
    "        start = time.time()\n",
    "        page = 0\n",
    "        if section + \".csv\" in os.listdir(directory):\n",
    "            News_DataFrame = pd.read_csv(section+\".csv\", encoding=\"utf-8-sig\", engine=\"python\")\n",
    "        else:\n",
    "            News_DataFrame = pd.DataFrame()\n",
    "\n",
    "        # 서버에서 뉴스 데이터 가져오기\n",
    "        flag = True\n",
    "        while(flag):\n",
    "            page += 1\n",
    "\n",
    "            # 제목형으로 정치 섹션 데이터 긁어오기\n",
    "            url = \"https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&listType=title&sid1=\" + Section_ID[\"Political\"] + \"&date=\" + date + \"&page=\" + str(page)\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "            resp = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # news_box 한 요소당 5개의 뉴스가 포함됨\n",
    "            news_box = soup.find_all(class_ = \"type02\")\n",
    "            for box in range(len(news_box)):\n",
    "                news = [i.get_text() for i in news_box[box].find_all(class_=\"nclicks(fls.list)\")]\n",
    "                urls = [i['href'] for i in news_box[box].find_all(class_=\"nclicks(fls.list)\")]\n",
    "                press = [i.get_text() for i in news_box[box].find_all(class_=\"writing\")]\n",
    "                upload = [ str(now.date()) + ' ' + str(now.hour - 1) + ':' + str(60 + (now.minute - int(i.get_text()[:-2]))) \n",
    "                          for i in news_box[box].find_all(class_=\"date is_new\") if i.get_text() not in '시간']\n",
    "\n",
    "                # 1시간이 넘어간 뉴스는 긁어오지 않음 (발견되면 크롤링 종료)\n",
    "                if len(upload) != 5:\n",
    "                    flag = False\n",
    "                    news = news[:len(upload)]\n",
    "                    urls = urls[:len(upload)]\n",
    "                    press = press[:len(upload)]\n",
    "\n",
    "                temp_df = pd.DataFrame({\"News\":news, \"Url\":urls, \"Press\":press, \"Time\":upload})\n",
    "                News_DataFrame = pd.concat([News_DataFrame, temp_df], axis=0, ignore_index=True)\n",
    "                News_DataFrame.dropna(axis=0, inplace=True)\n",
    "\n",
    "        # 섹션 별 크롤링 시간 측정 end\n",
    "        end = time.time()\n",
    "        total_crawling_time = total_crawling_time + (end - start)        \n",
    "        \n",
    "        # 데이터 프레임 csv 파일로 저장\n",
    "        title = section + \".csv\"\n",
    "        News_DataFrame.to_csv(title, sep=\",\", encoding='utf-8-sig', index=False)\n",
    "        print(\"Crawling \" + date + \" \" + str(now.hour) + \" \" + section + \" News :\", end-start)\n",
    "        \n",
    "    print(date + \" \" + str(now.hour) + \" total time :\", total_crawling_time)\n",
    "    print(date + \" \" + str(now.hour) + \" average Time :\", total_crawling_time / len(Section_ID))\n",
    "    print(\"────────────────────────────\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-09T03:23:07.312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automation Crawling Start : 2021-02-09 12:23:10\n",
      "Start Crawling : 2021-02-09 13\n",
      "Crawling 20210209 13 Political News : 0.8855631351470947\n",
      "Crawling 20210209 13 Economic News : 0.8524665832519531\n",
      "Crawling 20210209 13 Society News : 0.8570346832275391\n",
      "Crawling 20210209 13 Culture News : 0.861732006072998\n",
      "Crawling 20210209 13 International News : 0.9115674495697021\n",
      "Crawling 20210209 13 Science News : 0.8757727146148682\n",
      "20210209 13 total time : 5.244136571884155\n",
      "20210209 13 average Time : 0.8740227619806925\n",
      "────────────────────────────\n",
      "Start Crawling : 2021-02-09 14\n",
      "Crawling 20210209 14 Political News : 0.9305093288421631\n",
      "Crawling 20210209 14 Economic News : 0.8298301696777344\n",
      "Crawling 20210209 14 Society News : 0.8028016090393066\n",
      "Crawling 20210209 14 Culture News : 0.8267898559570312\n",
      "Crawling 20210209 14 International News : 0.8068444728851318\n",
      "Crawling 20210209 14 Science News : 0.8337705135345459\n",
      "20210209 14 total time : 5.030545949935913\n",
      "20210209 14 average Time : 0.8384243249893188\n",
      "────────────────────────────\n",
      "Start Crawling : 2021-02-09 15\n",
      "Crawling 20210209 15 Political News : 5.680638074874878\n",
      "Crawling 20210209 15 Economic News : 1.2247254848480225\n",
      "Crawling 20210209 15 Society News : 1.193354845046997\n",
      "Crawling 20210209 15 Culture News : 1.5887529850006104\n",
      "Crawling 20210209 15 International News : 1.3404133319854736\n",
      "Crawling 20210209 15 Science News : 1.645838737487793\n",
      "20210209 15 total time : 12.673723459243774\n",
      "20210209 15 average Time : 2.112287243207296\n",
      "────────────────────────────\n",
      "Start Crawling : 2021-02-09 16\n",
      "Crawling 20210209 16 Political News : 1.9371907711029053\n",
      "Crawling 20210209 16 Economic News : 1.9959986209869385\n",
      "Crawling 20210209 16 Society News : 1.977318525314331\n",
      "Crawling 20210209 16 Culture News : 2.0284533500671387\n",
      "Crawling 20210209 16 International News : 1.8751070499420166\n",
      "Crawling 20210209 16 Science News : 1.987738847732544\n",
      "20210209 16 total time : 11.801807165145874\n",
      "20210209 16 average Time : 1.9669678608576457\n",
      "────────────────────────────\n",
      "Start Crawling : 2021-02-09 17\n",
      "Crawling 20210209 17 Political News : 2.040726900100708\n",
      "Crawling 20210209 17 Economic News : 1.4919323921203613\n",
      "Crawling 20210209 17 Society News : 1.7104101181030273\n",
      "Crawling 20210209 17 Culture News : 1.0958192348480225\n",
      "Crawling 20210209 17 International News : 0.9766292572021484\n",
      "Crawling 20210209 17 Science News : 1.0447978973388672\n",
      "20210209 17 total time : 8.360315799713135\n",
      "20210209 17 average Time : 1.3933859666188557\n",
      "────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pause, sys\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"Automation Crawling Start : \" + str(now.date()) + \" \" + str(now.hour) + \":\" + str(now.minute) + \":\" + str(now.second))\n",
    "\n",
    "while True:\n",
    "    d = datetime.now()\n",
    "    t = d + timedelta(hours=1)\n",
    "    pause.until(datetime(t.year, t.month, t.day, t.hour, 0, 0))\n",
    "    crawling()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
