{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ë„¤ì´ë²„ ë‰´ìŠ¤ ê°œí¸ ë˜ê¸° ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T17:40:15.720120Z",
     "start_time": "2021-01-15T17:40:15.712139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_news(sectionid, date):\n",
    "    url = \"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId=\" + str(sectionid) + \"&date=\" + str(date)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    l = []\n",
    "    ranking_text = soup.find_all(class_ = 'ranking_text')\n",
    "    for item in ranking_text:\n",
    "        d = {}\n",
    "        d['LinkSrc'] = item.find('a')['href']\n",
    "        d['Title'] = item.find('a')['title']\n",
    "        d['Views'] = item.find(class_ = \"ranking_view\").get_text()\n",
    "        l.append(d)\n",
    "    \n",
    "    for link in l:\n",
    "        resp = requests.get(\"http://news.naver.com\" + link['LinkSrc'])\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        content = soup.find(id=\"articleBodyContents\")\n",
    "        link['Content'] = clean_text(content)\n",
    "        \n",
    "    df = pd.DataFrame(l)\n",
    "    return df\n",
    "    \n",
    "# text ì •ì œí•˜ê¸°\n",
    "def clean_text(text):\n",
    "    content = text.get_text()\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', content)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>â–¶â–½â™¡â—€â”@\\#$%&\\\\\\=\\(\\'\\\"â“’(\\n)(\\t)]', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"ğŸ‡²\\u200bğŸ‡®\\u200bğŸ‡±\\u200bğŸ‡±\\u200bğŸ‡®\\u200bğŸ‡ª\\u200b\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë™ì˜ìƒ ë‰´ìŠ¤ ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€\", \"\")\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë„¤ì´ë²„ ë‰´ìŠ¤ ê°œí¸ëœ ì´í›„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T17:05:43.786937Z",
     "start_time": "2021-01-15T17:05:43.708928Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    277\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    277\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-752c513fb0cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# error!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://news.naver.com/main/ranking/office.nhn?officeId=214\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.pareser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# error ë°œìƒ\n",
    "url = \"https://news.naver.com/main/ranking/office.nhn?officeId=214\" \n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html.pareser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ì–¸ë¡ ì‚¬ ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T02:46:40.213490Z",
     "start_time": "2021-01-25T02:46:40.210530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "press_ID = {\"MBC\" : \"214\", \"KBS\" : \"056\", \"SBS\" : \"055\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### í…ìŠ¤íŠ¸ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T03:58:35.037540Z",
     "start_time": "2021-01-19T03:58:35.031556Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    content = text.get_text()\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', content)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>â–¶â–½â™¡â—€â”@\\#$%&\\\\\\=\\(\\'\\\"â“’(\\n)(\\t)]', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"ğŸ‡²\\u200bğŸ‡®\\u200bğŸ‡±\\u200bğŸ‡±\\u200bğŸ‡®\\u200bğŸ‡ª\\u200b\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë™ì˜ìƒ ë‰´ìŠ¤                       ë‰´ìŠ¤ë°ìŠ¤í¬ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë™ì˜ìƒ ë‰´ìŠ¤                       ë‰´ìŠ¤íˆ¬ë°ì´ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì•µì»¤  \", \"\")\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë‰´ìŠ¤ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T01:39:42.572178Z",
     "start_time": "2021-01-16T01:39:31.761536Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# demo v0.1\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.naver.com/main/ranking/office.nhn?officeId=214&date=20210114\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "resp = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "ranking_box = soup.find_all(class_=\"rankingnews_box_inner\")\n",
    "l = []\n",
    "\n",
    "rank = ranking_text[0].find_all(class_ = \"list_ranking_num\")\n",
    "url_list = ranking_text[0].find_all(class_=\"list_content\")\n",
    "\n",
    "for num in range(20):\n",
    "    d = {}\n",
    "    d['Rank'] = rank[num].get_text()\n",
    "    d['URL'] = url_list[num].find('a')['href']\n",
    "    d['Title'] = url_list[num].find('a').get_text()\n",
    "    d['View'] = url_list[num].find(class_ = \"list_view\").get_text()\n",
    "    l.append(d)\n",
    "\n",
    "for link in l:\n",
    "    resp = requests.get(\"http://news.naver.com\" + link['URL'], headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    content = soup.find(id=\"articleBodyContents\")\n",
    "    link['Content'] = clean_text(content)\n",
    "\n",
    "df = pd.DataFrame(l)\n",
    "df.to_csv(\"20210112 MBC ë§ì´ ë³¸ ë‰´ìŠ¤.csv\", sep=\",\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rank = ranking_text[1].find_all(class_ = \"list_ranking_num\")\n",
    "url_list = ranking_text[1].find_all(class_=\"list_content\")\n",
    "\n",
    "for num in range(20):\n",
    "    d = {}\n",
    "    d['Rank'] = rank[num].get_text()\n",
    "    d['URL'] = url_list[num].find('a')['href']\n",
    "    d['Title'] = url_list[num].find('a').get_text()\n",
    "    d['Comment'] = url_list[num].find(class_ = \"list_comment nclicks('RBP.dcmtnwscmt')\").get_text()\n",
    "    l.append(d)\n",
    "\n",
    "for link in l:\n",
    "    resp = requests.get(\"http://news.naver.com\" + link['URL'], headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    content = soup.find(id=\"articleBodyContents\")\n",
    "    link['Content'] = clean_text(content)\n",
    "\n",
    "df = pd.DataFrame(l)\n",
    "df.to_csv(\"20210112 MBC ë§ì´ ë³¸ ë‰´ìŠ¤.csv\", sep=\",\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T02:32:51.876935Z",
     "start_time": "2021-01-16T02:32:43.395385Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# demo v0.2 (2021.01.14, MBC)\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.naver.com/main/ranking/office.nhn?officeId=214&date=20210114\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "resp = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "ranking_box = soup.find_all(class_=\"rankingnews_box_inner\")\n",
    "l = []\n",
    "\n",
    "for ranking_type in range(2):\n",
    "    ranking = ranking_box[ranking_type].find_all(class_=\"list_ranking_num\")\n",
    "    url_list = ranking_box[ranking_type].find_all(class_=\"list_content\")\n",
    "\n",
    "    for rank in range(20):\n",
    "        d = {}\n",
    "        d['Rank'] = ranking[rank].get_text()\n",
    "        d['URL'] = url_list[rank].find('a')['href']\n",
    "        d['Title'] = url_list[rank].find('a').get_text()\n",
    "        if (ranking_type == 0):\n",
    "            d['View'] = url_list[rank].find(class_=\"list_view\").get_text()\n",
    "        elif (ranking_type == 1):\n",
    "            d['Comment'] = url_list[rank].find(class_=\"list_comment nclicks('RBP.dcmtnwscmt')\").get_text()\n",
    "        l.append(d)\n",
    "\n",
    "for news in l:\n",
    "    resp = requests.get(\"https://news.naver.com\" + news['URL'], headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    content = soup.find(id=\"articleBodyContents\")\n",
    "    news['Content'] = clean_text(content)\n",
    "\n",
    "df = pd.DataFrame(l)\n",
    "df.to_csv(\"20210114_MBC_ranking_news.csv\", sep=\",\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:01:45.200473Z",
     "start_time": "2021-01-18T05:01:18.819787Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling MBC news : 10.095808267593384\n",
      "Crawling KBS news : 8.139859437942505\n",
      "Crawling SBS news : 8.135042905807495\n"
     ]
    }
   ],
   "source": [
    "# demo v0.3 (2021.01.15, MBC, KBS, SBS)\n",
    "import os\n",
    "os.chdir(r\"C:/Users/cjy89/NLP/Naver_news_crawling/\")\n",
    "press_ID = {\"MBC\" : \"214\", \"KBS\" : \"056\", \"SBS\" : \"055\"}\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    content = text.get_text()\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', content)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>â–¶â–½â™¡â—€â”@\\#$%&\\\\\\=\\(\\'\\\"â“’(\\n)(\\t)]', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"ğŸ‡²\\u200bğŸ‡®\\u200bğŸ‡±\\u200bğŸ‡±\\u200bğŸ‡®\\u200bğŸ‡ª\\u200b\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë™ì˜ìƒ ë‰´ìŠ¤                       ë‰´ìŠ¤ë°ìŠ¤í¬ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë™ì˜ìƒ ë‰´ìŠ¤                       ë‰´ìŠ¤íˆ¬ë°ì´ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì•µì»¤  \", \"\")\n",
    "    return cleaned_text\n",
    "\n",
    "for press in press_ID:\n",
    "    start = time.time()\n",
    "    url = \"https://news.naver.com/main/ranking/office.nhn?officeId=\" + press_ID[press] + \"&date=20210115\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    ranking_box = soup.find_all(class_=\"rankingnews_box_inner\")\n",
    "    l = []\n",
    "\n",
    "    for ranking_type in range(2):\n",
    "        ranking = ranking_box[ranking_type].find_all(class_=\"list_ranking_num\")\n",
    "        url_list = ranking_box[ranking_type].find_all(class_=\"list_content\")\n",
    "\n",
    "        for rank in range(20):\n",
    "            d = {}\n",
    "            d['Rank'] = ranking[rank].get_text()\n",
    "            d['URL'] = url_list[rank].find('a')['href']\n",
    "            d['Title'] = url_list[rank].find('a').get_text()\n",
    "            if (ranking_type == 0):\n",
    "                d['View'] = url_list[rank].find(class_=\"list_view\").get_text()\n",
    "            elif (ranking_type == 1):\n",
    "                d['Comment'] = url_list[rank].find(class_=\"list_comment nclicks('RBP.dcmtnwscmt')\").get_text()\n",
    "            l.append(d)\n",
    "\n",
    "    for news in l:\n",
    "        resp = requests.get(\"https://news.naver.com\" + news['URL'], headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        content = soup.find(id=\"articleBodyContents\")\n",
    "        news['Content'] = clean_text(content)\n",
    "\n",
    "    df = pd.DataFrame(l)\n",
    "    title = press + \"/20210115_\" + press + \"_ranking_news.csv\"\n",
    "    df.to_csv(title, sep=\",\", index=False, encoding=\"utf-8-sig\")\n",
    "    end = time.time()\n",
    "    print(\"Crawling \" + press + \" news :\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T07:49:55.045036Z",
     "start_time": "2021-02-01T07:49:55.034064Z"
    },
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "# demo v0.4 ë‚ ì§œë¥¼ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ í¬ë¡¤ë§\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\cjy89\\NLP\\Project_news_crawling\\Naver\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# press_ID = {\"MBC\":\"214\"}\n",
    "press_ID = {\"KBS\": \"056\", \"SBS\": \"055\", \"JTBC\": \"437\"}\n",
    "\n",
    "\"\"\"\n",
    "def clean_text(text):\n",
    "    content = text.get_text()\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', content)\n",
    "    cleaned_text = re.sub(\n",
    "        '[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>â–¶â–½â™¡â—€â”@\\#$%&\\\\\\=\\(\\'\\\"â“’(\\n)(\\t)]', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\n",
    "        \"ğŸ‡²\\u200bğŸ‡®\\u200bğŸ‡±\\u200bğŸ‡±\\u200bğŸ‡®\\u200bğŸ‡ª\\u200b\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€\", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\n",
    "        \"ë™ì˜ìƒ ë‰´ìŠ¤                       ë‰´ìŠ¤ë°ìŠ¤í¬ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\n",
    "        \"ë™ì˜ìƒ ë‰´ìŠ¤                       ë‰´ìŠ¤íˆ¬ë°ì´ \", \"\")\n",
    "    cleaned_text = cleaned_text.replace(\"ì•µì»¤  \", \"\")\n",
    "    return cleaned_text\n",
    "\"\"\"\n",
    "\n",
    "# 8ìë¦¬ë¡œ ëœ ë‚ ì§œë¥¼ ì…ë ¥í•˜ë©´ í•´ë‹¹ ë‚ ì§œì˜ ranking newsë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
    "def get_ranking_news(date):\n",
    "    total_time = 0\n",
    "    \n",
    "    # ì–¸ë¡ ì‚¬ ìˆœ\n",
    "    for press in press_ID:\n",
    "        start = time.time()\n",
    "        url = \"https://news.naver.com/main/ranking/office.nhn?officeId=\" + press_ID[press] + \"&date=\" + str(date)\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        ranking_box = soup.find_all(class_=\"rankingnews_box_inner\")\n",
    "        l = []\n",
    "\n",
    "        # ì¡°íšŒìˆ˜ -> ëŒ“ê¸€ ìˆ˜\n",
    "        for ranking_type in range(2):\n",
    "            ranking = ranking_box[ranking_type].find_all(class_=\"list_ranking_num\")\n",
    "            url_list = ranking_box[ranking_type].find_all(class_=\"list_content\")\n",
    "\n",
    "            # ë­í‚¹ ìˆœ (1 ~ 20)\n",
    "            for rank in range(20):\n",
    "                d = {}\n",
    "                d['Date'] = int(date)\n",
    "                d['Press'] = press\n",
    "                d['Rank'] = ranking[rank].get_text()\n",
    "                d['URL'] = url_list[rank].find('a')['href']\n",
    "                d['Title'] = url_list[rank].find('a').get_text()\n",
    "                if (ranking_type == 0):\n",
    "                    d['View'] = url_list[rank].find(class_=\"list_view\").get_text()\n",
    "                elif (ranking_type == 1):\n",
    "                    d['Comment'] = url_list[rank].find(class_=\"list_comment nclicks('RBP.dcmtnwscmt')\").get_text()\n",
    "                l.append(d)\n",
    "\n",
    "        # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        for news in l:\n",
    "            resp = requests.get(\"https://news.naver.com\" + news['URL'], headers=headers)\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            contents = soup.find(id=\"articleBodyContents\").get_text()\n",
    "            news['Content'] = re.sub('[\\{\\}\\[\\]\\/?\\(\\);:|*~`!^\\-_+<>â–¶â–½â™¡â—€â”@\\#$&\\\\\\=\\'\\\"â“’(\\n)(\\t)]', ' ', contents)\n",
    "            # news['Content'] = clean_text(content)\n",
    "\n",
    "        df = pd.DataFrame(l)\n",
    "        title = press + \"/\" + str(date) + \"_\" + press + \"_ranking_news.csv\"\n",
    "        df.to_csv(title, sep=\",\", index=False, encoding=\"utf-8-sig\")\n",
    "        end = time.time()\n",
    "        total_time += end - start\n",
    "        print(\"Crawling \" + str(date) + \" \" + press + \" news :\", end - start)\n",
    "    print(\"Total time :\", total_time)\n",
    "    print(\"Average time : \", total_time/len(press_ID))\n",
    "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T08:03:02.530712Z",
     "start_time": "2021-02-01T07:49:56.053882Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 20210101 KBS news : 9.752766847610474\n",
      "Crawling 20210101 SBS news : 8.970605373382568\n",
      "Crawling 20210101 JTBC news : 9.061842679977417\n",
      "Total time : 27.78521490097046\n",
      "Average time :  9.261738300323486\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210102 KBS news : 8.599104166030884\n",
      "Crawling 20210102 SBS news : 8.933659791946411\n",
      "Crawling 20210102 JTBC news : 8.457802057266235\n",
      "Total time : 25.99056601524353\n",
      "Average time :  8.663522005081177\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210103 KBS news : 10.385263919830322\n",
      "Crawling 20210103 SBS news : 8.947070598602295\n",
      "Crawling 20210103 JTBC news : 8.91811490058899\n",
      "Total time : 28.250449419021606\n",
      "Average time :  9.416816473007202\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210104 KBS news : 8.9640531539917\n",
      "Crawling 20210104 SBS news : 9.114179611206055\n",
      "Crawling 20210104 JTBC news : 9.843366861343384\n",
      "Total time : 27.921599626541138\n",
      "Average time :  9.307199875513712\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210105 KBS news : 8.779188394546509\n",
      "Crawling 20210105 SBS news : 8.281689882278442\n",
      "Crawling 20210105 JTBC news : 8.868164300918579\n",
      "Total time : 25.92904257774353\n",
      "Average time :  8.643014192581177\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210106 KBS news : 8.101884126663208\n",
      "Crawling 20210106 SBS news : 7.734166145324707\n",
      "Crawling 20210106 JTBC news : 8.344053268432617\n",
      "Total time : 24.180103540420532\n",
      "Average time :  8.06003451347351\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210107 KBS news : 7.9201719760894775\n",
      "Crawling 20210107 SBS news : 8.035281419754028\n",
      "Crawling 20210107 JTBC news : 8.464389562606812\n",
      "Total time : 24.419842958450317\n",
      "Average time :  8.139947652816772\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210108 KBS news : 8.353309154510498\n",
      "Crawling 20210108 SBS news : 8.360011339187622\n",
      "Crawling 20210108 JTBC news : 7.704000473022461\n",
      "Total time : 24.41732096672058\n",
      "Average time :  8.13910698890686\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210109 KBS news : 9.373552560806274\n",
      "Crawling 20210109 SBS news : 7.778036594390869\n",
      "Crawling 20210109 JTBC news : 7.902531147003174\n",
      "Total time : 25.054120302200317\n",
      "Average time :  8.351373434066772\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210110 KBS news : 7.7811973094940186\n",
      "Crawling 20210110 SBS news : 8.394603252410889\n",
      "Crawling 20210110 JTBC news : 7.9871790409088135\n",
      "Total time : 24.16297960281372\n",
      "Average time :  8.05432653427124\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210111 KBS news : 8.07268476486206\n",
      "Crawling 20210111 SBS news : 7.904094457626343\n",
      "Crawling 20210111 JTBC news : 8.112919330596924\n",
      "Total time : 24.089698553085327\n",
      "Average time :  8.029899517695108\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210112 KBS news : 8.091586828231812\n",
      "Crawling 20210112 SBS news : 8.535082817077637\n",
      "Crawling 20210112 JTBC news : 9.33979868888855\n",
      "Total time : 25.966468334197998\n",
      "Average time :  8.655489444732666\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210113 KBS news : 9.009406089782715\n",
      "Crawling 20210113 SBS news : 8.875461339950562\n",
      "Crawling 20210113 JTBC news : 8.425336360931396\n",
      "Total time : 26.310203790664673\n",
      "Average time :  8.770067930221558\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210114 KBS news : 8.802478790283203\n",
      "Crawling 20210114 SBS news : 8.203390836715698\n",
      "Crawling 20210114 JTBC news : 8.449881315231323\n",
      "Total time : 25.455750942230225\n",
      "Average time :  8.485250314076742\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210115 KBS news : 9.185250043869019\n",
      "Crawling 20210115 SBS news : 8.537165641784668\n",
      "Crawling 20210115 JTBC news : 8.19019079208374\n",
      "Total time : 25.912606477737427\n",
      "Average time :  8.637535492579142\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210116 KBS news : 8.462743043899536\n",
      "Crawling 20210116 SBS news : 8.521527290344238\n",
      "Crawling 20210116 JTBC news : 8.528228282928467\n",
      "Total time : 25.51249861717224\n",
      "Average time :  8.504166205724081\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210117 KBS news : 8.563339948654175\n",
      "Crawling 20210117 SBS news : 10.031767845153809\n",
      "Crawling 20210117 JTBC news : 8.48665189743042\n",
      "Total time : 27.081759691238403\n",
      "Average time :  9.027253230412802\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210118 KBS news : 8.912639856338501\n",
      "Crawling 20210118 SBS news : 8.309457778930664\n",
      "Crawling 20210118 JTBC news : 8.39600944519043\n",
      "Total time : 25.618107080459595\n",
      "Average time :  8.539369026819864\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210119 KBS news : 8.971197605133057\n",
      "Crawling 20210119 SBS news : 8.319584131240845\n",
      "Crawling 20210119 JTBC news : 8.462327480316162\n",
      "Total time : 25.753109216690063\n",
      "Average time :  8.584369738896688\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210120 KBS news : 8.132288694381714\n",
      "Crawling 20210120 SBS news : 8.278854370117188\n",
      "Crawling 20210120 JTBC news : 7.788194417953491\n",
      "Total time : 24.199337482452393\n",
      "Average time :  8.06644582748413\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210121 KBS news : 8.77254843711853\n",
      "Crawling 20210121 SBS news : 8.187190294265747\n",
      "Crawling 20210121 JTBC news : 8.098698139190674\n",
      "Total time : 25.05843687057495\n",
      "Average time :  8.35281229019165\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210122 KBS news : 11.264352560043335\n",
      "Crawling 20210122 SBS news : 8.226301670074463\n",
      "Crawling 20210122 JTBC news : 8.018795013427734\n",
      "Total time : 27.509449243545532\n",
      "Average time :  9.169816414515177\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210123 KBS news : 8.045024633407593\n",
      "Crawling 20210123 SBS news : 8.069119215011597\n",
      "Crawling 20210123 JTBC news : 8.051504850387573\n",
      "Total time : 24.165648698806763\n",
      "Average time :  8.055216232935587\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210124 KBS news : 7.72951078414917\n",
      "Crawling 20210124 SBS news : 10.16853666305542\n",
      "Crawling 20210124 JTBC news : 8.032679557800293\n",
      "Total time : 25.930727005004883\n",
      "Average time :  8.643575668334961\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210125 KBS news : 8.336347103118896\n",
      "Crawling 20210125 SBS news : 8.273195028305054\n",
      "Crawling 20210125 JTBC news : 8.106472253799438\n",
      "Total time : 24.71601438522339\n",
      "Average time :  8.238671461741129\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210126 KBS news : 8.133248090744019\n",
      "Crawling 20210126 SBS news : 7.977166414260864\n",
      "Crawling 20210126 JTBC news : 8.916840076446533\n",
      "Total time : 25.027254581451416\n",
      "Average time :  8.342418193817139\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210127 KBS news : 8.124829292297363\n",
      "Crawling 20210127 SBS news : 8.152368307113647\n",
      "Crawling 20210127 JTBC news : 7.996850252151489\n",
      "Total time : 24.2740478515625\n",
      "Average time :  8.091349283854166\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210128 KBS news : 8.486669063568115\n",
      "Crawling 20210128 SBS news : 8.195515394210815\n",
      "Crawling 20210128 JTBC news : 7.73069429397583\n",
      "Total time : 24.41287875175476\n",
      "Average time :  8.13762625058492\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210129 KBS news : 8.544337034225464\n",
      "Crawling 20210129 SBS news : 8.189895391464233\n",
      "Crawling 20210129 JTBC news : 7.897546291351318\n",
      "Total time : 24.631778717041016\n",
      "Average time :  8.210592905680338\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210130 KBS news : 8.233208656311035\n",
      "Crawling 20210130 SBS news : 7.881181955337524\n",
      "Crawling 20210130 JTBC news : 7.804587125778198\n",
      "Total time : 23.918977737426758\n",
      "Average time :  7.972992579142253\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Crawling 20210131 KBS news : 7.728961706161499\n",
      "Crawling 20210131 SBS news : 7.433881044387817\n",
      "Crawling 20210131 JTBC news : 7.610538721084595\n",
      "Total time : 22.77338147163391\n",
      "Average time :  7.591127157211304\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'printf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9edb4f8e798f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mget_ranking_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20210101\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprintf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"complete crawling\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'printf' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(31):\n",
    "    get_ranking_news(20210101+i)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "90px",
    "width": "184px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 565,
   "position": {
    "height": "71px",
    "left": "1528px",
    "right": "20px",
    "top": "184px",
    "width": "280px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
